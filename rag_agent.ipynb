{"cells":[{"cell_type":"markdown","source":["#Environment setup"],"metadata":{"id":"-DQu0A6YJg82"}},{"cell_type":"markdown","source":["Necessary libraries"],"metadata":{"id":"Kc7w0lfjLCZj"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8tVHzsehKJql"},"outputs":[],"source":["!pip install langchain langchain_community faiss-cpu"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"aY3IW-47KUp2","executionInfo":{"status":"ok","timestamp":1744467689308,"user_tz":-180,"elapsed":860,"user":{"displayName":"Андрей","userId":"13514498339036771192"}}},"outputs":[],"source":["from google.colab import files\n","import zipfile\n","import os\n","import faiss\n","from langchain.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_community.document_loaders import TextLoader\n","from langchain_community.vectorstores import FAISS\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain.chains import create_retrieval_chain\n","from langchain_community.llms import HuggingFaceEndpoint\n","from langchain_community.chat_models.huggingface import ChatHuggingFace\n","from langchain_community.document_loaders.word_document import Docx2txtLoader\n","from langchain_community.document_loaders import DirectoryLoader"]},{"cell_type":"markdown","source":["Global variables"],"metadata":{"id":"DTDCssf_LJyT"}},{"cell_type":"code","source":["db_path = \"/content/data\"\n","api_token = \"your api token here\""],"metadata":{"id":"ENsY9s5HLNJq","executionInfo":{"status":"ok","timestamp":1744467689308,"user_tz":-180,"elapsed":1,"user":{"displayName":"Андрей","userId":"13514498339036771192"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#Data uploading"],"metadata":{"id":"HHzzLDrcTJdj"}},{"cell_type":"code","source":["uploaded = files.upload()"],"metadata":{"id":"hYrVM0OJTMRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/data.zip -d /content/data"],"metadata":{"id":"SMKVmtznTgXt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Body part"],"metadata":{"id":"Rvzx8isJKco0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jW3i3TGQ-dzC"},"outputs":[],"source":["# Load all .txt files from a directory\n","docs = []\n","\n","for root, dirs, files in os.walk(db_path):\n","    for f in files:\n","        if f.endswith('.txt'):\n","            file_path = os.path.join(root, f)\n","            loader = TextLoader(file_path)\n","            docs.extend(loader.load())\n","\n","# Split text into chunks\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=500,\n","    chunk_overlap=100, )\n","documents = text_splitter.split_documents(docs)\n","\n","# Define the embedding model\n","model_name = \"BAAI/bge-m3\"\n","model_kwargs = {'device': 'cuda'}\n","encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n","embeddings = HuggingFaceBgeEmbeddings(\n","    model_name=model_name,\n","    model_kwargs=model_kwargs,\n","    encode_kwargs=encode_kwargs,\n",")\n","\n","# Create the vector store\n","vector = FAISS.from_documents(documents, embeddings)\n","\n","# Define a retriever interface\n","retriever = vector.as_retriever()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50j9HvtTqLYF"},"outputs":[],"source":["# Define LLM\n","llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n","                          task=\"text-generation\",\n","                          huggingfacehub_api_token=api_token,\n","                          max_new_tokens = 500,)\n","model = ChatHuggingFace(llm=llm)\n","\n","# Define prompt template\n","prompt = ChatPromptTemplate.from_template(\"\"\"Старайся отвечать вежливо и точно. Ответь на поставленный вопрос в соответствии с контекстом\n","\n","<context>\n","{context}\n","</context>\n","\n","Вопрос: {input}\"\"\")\n","\n","# Create a retrieval chain to answer questions\n","document_chain = create_stuff_documents_chain(model, prompt)\n","retrieval_chain = create_retrieval_chain(retriever, document_chain)\n","query = '''\n","Как позвонить на Горячую линию Cекретариата Российского совета олимпиад школьников?\n","'''\n","response = retrieval_chain.invoke({\"input\": query})\n","print(response[\"answer\"])"]},{"cell_type":"code","source":[],"metadata":{"id":"BYeMcZD6Yvnn"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOlgCZyWhgPoq/dXFvcJ5Sq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}